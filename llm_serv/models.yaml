#
# This file contains the models that are available in the LLM service.
# It is used to populate the registry and the API.
# 

PROVIDERS:
  AWS:
    name: AWS
    config: {}
  AZURE:
    name: AZURE
    config: {}
  OPENAI:
    name: OPENAI
    config: {}
  MISTRAL:
    name: MISTRAL
    config: {}
  ANTHROPIC:
    name: ANTHROPIC
    config: {}
  GROK:
    name: GROK
    config: {}

MODELS:
  AZURE/gpt-4o:    
    internal_model_id: gpt-4o
    max_tokens: 128000
    max_output_tokens: 16000
    capabilities:
      image_support: true
      document_support: true
    config: {}

  AZURE/gpt-4o-mini:
    internal_model_id: gpt-4o-mini
    max_tokens: 128000
    max_output_tokens: 16000
    capabilities:
      image_support: true
      document_support: true
    config: {}

  OPENAI/gpt-4o:
    internal_model_id: gpt-4o
    max_tokens: 128000
    max_output_tokens: 16000
    capabilities:
      image_support: true
      document_support: true
    config: {}

  OPENAI/gpt-4o-mini:
    internal_model_id: gpt-4o-mini
    max_tokens: 128000
    max_output_tokens: 16000
    capabilities:
      image_support: true
      document_support: true
    config: {}

  AWS/claude-3-5-sonnet:
    internal_model_id: anthropic.claude-3-5-sonnet-20240620-v1:0
    max_tokens: 200000
    max_output_tokens: 4096
    capabilities:
      image_support: false
      document_support: false
    config: {}
    
  AWS/eu-claude-3-5-sonnet:
    internal_model_id: anthropic.claude-3-5-sonnet-20240620-v1:0
    max_tokens: 200000
    max_output_tokens: 4096
    capabilities:
      image_support: false
      document_support: false
    config: {}

  AWS/claude-3-haiku:
    internal_model_id: anthropic.claude-3-haiku-20240307-v1:0
    max_tokens: 200000
    max_output_tokens: 4096
    capabilities:
      image_support: true
      document_support: false
    config: {}
  